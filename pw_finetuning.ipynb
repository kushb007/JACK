{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2867390-3982-4277-adc9-d24f6f8d43d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc5897-0ee3-4e88-a268-11c0818c708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\n",
    "# should return true\n",
    "print(tf.test.is_built_with_cuda())\n",
    "# will list your available gpu\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f9bfa-a400-45f0-8d50-03b9ac24bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "QLoRA = True\n",
    "if QLoRA:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        target_modules=\"all-linear\",\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "else:\n",
    "    lora_config = None\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,quantization_config=quantization_config)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d802d7a5-6ad3-4e69-aae2-1b4a71f5ba72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db1722-0655-468a-8894-ac7b89183580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"deepmind/code_contests\",cache_dir=\"D:/data\",split=\"train\")\n",
    "#dataset = load_dataset(\"imdb\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c17d4a2-27d4-468c-bab4-9f73ab5d0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"description\",\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d792ca-f5ed-4835-a66e-1ff59c68b789",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2d827-d656-41c7-8cf9-03e8db170835",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./models\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./models\",quantization_config=quantization_config)\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c947786f-6bbe-441a-a9f3-0f00648a1908",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = pipe(\"You have five duelists on your team\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5168e1d3-d685-4c5c-a727-d20dc19463c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a problem writer part of a team of great and creative problem writers. As a team you should create a clear thought by thought reasoning to create a problem description for a competitive programming problem. The problem should be straightforward and involve algorithmic thinking and edge cases. You will be given a competitive programming problem from another member of your team and you must write a new revision that is more readable. It must contain the same algorithm as the input's and fix any issues.\"},\n",
    "    {\"role\": \"user\", \"content\": problem},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7378e725-1554-4188-9650-42b30cf533a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c36c843da1c43109d5e66b89a193bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "C:\\Users\\kushb\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:660: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"get_current_temperature\", \"parameters\": {\"location\": \"Paris, France\"}}<|eom_id|>\n"
     ]
    }
   ],
   "source": [
    "# First, define a tool\n",
    "def get_current_temperature(location: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the current temperature at a location.\n",
    "    \n",
    "    Args:\n",
    "        location: The location to get the temperature for, in the format \"City, Country\"\n",
    "    Returns:\n",
    "        The current temperature at the specified location in the specified units, as a float.\n",
    "    \"\"\"\n",
    "    return 22.  # A real function should probably actually get the temperature!\n",
    "\n",
    "# Next, create a chat and apply the chat template\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "model = model.to(\"cuda:0\")\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "out = model.generate(**inputs, max_new_tokens=128)\n",
    "print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79601b41-d13f-46e2-a036-9e235be49ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8beab8d3-5f4f-4bec-9d64-20adcd0e33bc",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc276e22-ddbf-4d82-9dd7-aacbb0c228a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "26bd3730ff40494d9eaec09e127bc833": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9d71096a899642af8a65d22ba712bea0",
       "style": "IPY_MODEL_556e15a4df39404a8a0d39f2cdd5d7ab",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "2b0530ad071245a884bb2bd944e98cba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "556e15a4df39404a8a0d39f2cdd5d7ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7c36c843da1c43109d5e66b89a193bda": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_26bd3730ff40494d9eaec09e127bc833",
        "IPY_MODEL_df87abe166454ff5bb2fb66a4a54355f",
        "IPY_MODEL_cc99f4f05bb2475eb434993a03c88db0"
       ],
       "layout": "IPY_MODEL_2b0530ad071245a884bb2bd944e98cba"
      }
     },
     "9d71096a899642af8a65d22ba712bea0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a28750e7d5b147b9ac3cddab3f918004": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a6a98703641a41cf840a090ec7205629": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a8d80142895e47f9910fcb6c1327fd36": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cc99f4f05bb2475eb434993a03c88db0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a28750e7d5b147b9ac3cddab3f918004",
       "style": "IPY_MODEL_a6a98703641a41cf840a090ec7205629",
       "value": " 4/4 [00:39&lt;00:00,  9.73s/it]"
      }
     },
     "df87abe166454ff5bb2fb66a4a54355f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_a8d80142895e47f9910fcb6c1327fd36",
       "max": 4,
       "style": "IPY_MODEL_f7944583d81648d798bd358130341ef0",
       "value": 4
      }
     },
     "f7944583d81648d798bd358130341ef0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
